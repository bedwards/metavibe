# Finding What's Worth Reading

Dave Winer started publishing an XML version of his blog in December 1997. The format spread to other sites, evolving through several versions before stabilizing as RSS—Really Simple Syndication. For a brief golden era in the mid-2000s, RSS represented a genuinely decentralized approach to web content. Subscribe to a feed, see everything published to it, no algorithm deciding what deserves your attention.

That era ended, or seemed to end, when Google Reader shut down in 2013. The shutdown wasn't about declining usage—engineers who worked on the product said it was successful and growing. It was sacrificed to push users toward Google Plus, a social network that never gained traction and eventually died itself. But the damage was done. Without Google Reader to educate users about RSS, adoption plummeted. Browsers removed RSS features. Platforms stopped promoting their feeds.

Yet RSS survived. Not as a mainstream technology, but as infrastructure that quietly powers much of the web. Every Substack publication has an RSS feed at a predictable URL. WordPress sites generate feeds automatically. Podcasts are distributed via RSS. The protocol refuses to die because it solves a real problem: letting people subscribe to content without surrendering control to a platform.

Building content discovery for a personal reading library means building on RSS. The alternative—scraping websites directly—is fragile, legally questionable, and constantly breaking. RSS feeds are explicitly designed for machine consumption. They follow documented formats. They represent a contract between publisher and subscriber: you can have this content in a structured format.

The vibe coding approach to RSS integration starts with describing what you want: a service that takes a feed URL, fetches it, parses the XML, and returns structured data about the feed and its items. Claude generates a feed service using established libraries like rss-parser that handle the parsing complexities. The first iteration usually works for simple feeds.

Complications emerge with real-world feeds. Some feeds are Atom instead of RSS, using slightly different XML structures. The parsing library handles both, but you learn to normalize the differences in your data model. Some feeds include full article content. Others provide only excerpts, requiring a separate fetch to get the complete text. Some feeds haven't been updated in years and return stale data. Others update constantly with hundreds of new items.

The pattern that worked best was capturing everything the feed provides while flagging anomalies for investigation. Don't reject feeds that deviate from expectations. Store what you get. When something looks wrong—a feed with no items, a feed that fails to parse—log the error and continue. The scheduler will try again later.

Substack feeds deserve special attention because Substack has become the dominant platform for long-form newsletters. Every Substack publication follows the same URL pattern: the publication name followed by the standard feed path. Given a Substack URL, you can construct the feed URL programmatically. The technique for adding this to your library is describing the pattern to Claude: given a URL that contains the Substack domain, extract the publication name and construct the feed URL.

Feed discovery extends beyond explicit RSS URLs. Many websites include link elements in their HTML that point to their feeds. A discovery service can fetch a webpage, parse the HTML looking for these link elements, and return the feeds it finds. This works for sites that have feeds but don't advertise them prominently. The technique involves describing HTML parsing to Claude: fetch this URL, find link elements with RSS or Atom types, extract the href attributes.

The scheduler that fetches feeds periodically required more thought than expected. The naive approach—fetch all feeds every thirty minutes—works until you have enough feeds that the fetches take longer than thirty minutes. Better to fetch feeds independently, tracking when each was last updated and prioritizing feeds that haven't been fetched recently.

Error handling in the scheduler became its own mini-project. Feeds fail for many reasons: network timeouts, invalid certificates, rate limiting, servers that are down. The scheduler needs to record these errors without crashing, continue processing other feeds, and retry failed feeds with backoff. This is exactly the kind of tedious, important work that vibe coding handles well. Describe the error handling behavior you want, let Claude generate the implementation, test it against real failure scenarios.

OPML emerged as an important format for feed management. OPML is an XML format that stores lists of RSS feeds, commonly used to export and import subscriptions between RSS readers. Supporting OPML import means users can migrate their existing subscriptions. Supporting OPML export means users can leave for another reader without losing their feed list. Both are straightforward to implement once you understand the format—Claude can generate parsers and generators from format descriptions.

The discovery interface in the frontend brought its own challenges. Users expect to paste any URL and have the system figure out the feed. They shouldn't need to know RSS exists. The interface should accept a blog URL, a Substack URL, or a direct feed URL, discover what's available, and present options to subscribe. This required chaining the discovery logic: try to parse the URL as a feed directly, try to discover feeds from the HTML if that fails, try common feed paths like slash feed or slash rss if discovery returns nothing.

Feedback during discovery matters for user experience. Network requests take time. The interface should indicate that discovery is happening, show results as they become available, and provide clear error messages when discovery fails. The vibe coding approach here was describing the user experience rather than the implementation: show a loading state while checking for feeds, display found feeds with subscribe buttons, show a helpful message if no feeds are found.

One technique that proved valuable was building the discovery features incrementally rather than all at once. Start with the simplest case: the user provides a direct feed URL, you fetch and subscribe. Then add Substack URL detection. Then add HTML discovery. Then add common path probing. Each addition is a focused vibe coding session that extends existing functionality. The alternative—trying to build comprehensive discovery from the start—leads to sessions that sprawl and stall.

The feeds table in the database accumulated fields over time as edge cases emerged. Last fetched timestamp for the scheduler. Fetch error field to record failures. Active flag to pause feeds without deleting them. Site URL and favicon for display. Each addition came from encountering a real need while using the library.

Content discovery is the foundation everything else builds on. Without feeds flowing into the system, there's nothing to read, nothing to organize, nothing to convert to speech. Getting discovery right—reliable, automatic, error-tolerant—determines whether the library becomes a daily tool or an abandoned project. The investment in solid feed handling pays compound interest across every other feature.
